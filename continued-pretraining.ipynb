{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6N_eDkWXBzk"
   },
   "source": [
    "# Teach a Small LLM French with Unsloth (CPT on Wikipedia-FR)\n",
    "\n",
    "This Colab shows how to continually pretrain (CPT) a lightweight LLM to improve its French fluency French Wikipedia. We use Unsloth to load the model in 4-bit (low VRAM) while computing in a safer precision, and fine-tune with LoRA adapters so training fits on a Colab T4.\n",
    "\n",
    "## What we’ll do\n",
    "* Set up Unsloth + Hugging Face ecosystem on Colab.\n",
    "* Load a compact base model (unsloth/gemma-3-1b-pt…) in 4-bit for speed/VRAM.\n",
    "* Run a baseline French generation (before training).\n",
    "* Prepare French Wikipedia texts with a clean template and EOS tokens.\n",
    "* Fine-tune via CPT (LoRA)\n",
    "* Generate again after CPT\n",
    "\n",
    "## Dataset\n",
    "**Source**: wikimedia/wikipedia snapshot 20231101.fr\n",
    "* We take exactly 1% of the dataset, then a tiny slice from that for eval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxCt1VqTUUvi"
   },
   "source": [
    "# Install Libraries\n",
    "* Installs Unsloth and the Hugging Face stack plus bitsandbytes for 4-/8-bit loading and the Hub client. This sets up everything needed for lightweight fine-tuning and inference on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sc6TUily3QGf",
    "outputId": "8d14c138-4092-4683-ead6-af3adf9b7773"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m351.3/351.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install -U \"unsloth\" \"unsloth_zoo\" \"transformers>=4.45.0\" \"trl>=0.11.4\" \"datasets\" \"accelerate\" \"peft\" \"bitsandbytes\" \"huggingface_hub\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdkDhV6TUhAf"
   },
   "source": [
    "# Environment & GPU check\n",
    "\n",
    "* Import Python deps and prints basic runtime info.\n",
    "* Check whether a CUDA GPU is available and reports PyTorch/CUDA versions and the GPU name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OWnfmCWC6NPv",
    "outputId": "64ba829b-e4ac-4c0f-fbf3-e863af5c674b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla T4 (UUID: GPU-df067857-72b4-b59d-e854-639288c6323d)\n",
      "PyTorch: 2.8.0+cu126 | CUDA: 12.6 | Device: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "\n",
    "# Check GPU\n",
    "!nvidia-smi -L || echo \"No GPU detected.\"\n",
    "if torch.cuda.is_available():\n",
    "    print(\"PyTorch:\", torch.__version__, \"| CUDA:\", torch.version.cuda, \"| Device:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"PyTorch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available(), \"| Using CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_1IxAUFUsLy"
   },
   "source": [
    "# Global Configuration\n",
    "\n",
    "* Define our base model, sequence length, precision, and random seed\n",
    "* using unsloth/gemma-3-1b-pt-unsloth-bnb-4bit for CPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "_M4weurF6S6t"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "BASE_MODEL = \"unsloth/gemma-3-1b-pt-unsloth-bnb-4bit\"\n",
    "MAX_SEQ_LEN = 512\n",
    "LOAD_IN_4BIT = True\n",
    "DTYPE = torch.bfloat16 if is_bfloat16_supported() else torch.float16\n",
    "SEED = 3407"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xShwoi0GU91w"
   },
   "source": [
    "# Load Model (4-bit)\n",
    "\n",
    "* Loads the model with 4-bit weights but float32 compute for stability, sets a pad token, and puts the model into a fast inference graph with flash-attention-2 disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ymPDEsll6oe8",
    "outputId": "3bbde99c-a592-4e25-f46b-172c75ecef82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.2: Fast Gemma3 patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n",
      "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name     = BASE_MODEL,\n",
    "    max_seq_length = MAX_SEQ_LEN,\n",
    "    dtype          = torch.float32,\n",
    "    load_in_4bit   = True,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "try:\n",
    "    FastLanguageModel.for_inference(model, use_flash_attention_2=False)\n",
    "except TypeError:\n",
    "    FastLanguageModel.for_inference(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geJ2qvMRVLJ1"
   },
   "source": [
    "# Baseline Inference (before training)\n",
    "\n",
    "* Run a quick French generation to see how the base model behaves before CPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50IagJ2i61D-",
    "outputId": "83093bd3-48da-46f4-de48-8650ef6347fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " se côtoient les langues, les cultures et les valeurs.\n",
      "\n",
      "L'anglais est une langue de communication très importante dans le monde moderne. Grâce à l'anglais, les gens peuvent communiquer avec des personnes d'autres pays. Ils peuvent étudier à l'étranger, travailler dans d'autres pays et voyager à travers le monde. Les langues étrangères sont très importantes pour les gens qui\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "prompt = \"Complète cette phrase en bon français, de façon naturelle : La francophonie est un espace où\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=80,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    streamer=streamer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NxjeixTSyDN"
   },
   "source": [
    "## Translation of Baseline Prompt and Response\n",
    "\n",
    "* Prompt: Complete this sentence in good French, in a natural way: \"The Francophonie is a space where\"\n",
    "\n",
    "  * Response: Languages, cultures and values ​​coexist. English is a very important language of communication in the modern world. Thanks to English, people can communicate with people from other countries. They can study abroad, work in other countries, and travel the world. Foreign languages ​​are very important for people who\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtYDcMjIVTz_"
   },
   "source": [
    "# Enable LoRA Fine-Tuning\n",
    "\n",
    "* Wrap the model with PEFT/LoRA adapters on attention/MLP projections and lm_head\n",
    "* Gradient checkpointing reduces memory usage; RSLoRA improves efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNaAka3V7o3P",
    "outputId": "e5cac44e-11c0-49d2-d853-9a64ce8623e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:693: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# If you hit OOM, first remove \"embed_tokens\" from target_modules.\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "        \"gate_proj\",\"up_proj\",\"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=SEED,\n",
    "    use_rslora=True,\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xP_gEy2UWv9m"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wp7MRyU4Ve37"
   },
   "source": [
    "# French Wikipedia Formatting\n",
    "\n",
    "* Create a French prompt template for Wikipedia pages and a formatting_prompts_func that merges title + article and appends the EOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "La09qGFB7vHN"
   },
   "outputs": [],
   "source": [
    "# French template used for training:\n",
    "wikipedia_prompt = \"\"\"Article Wikipédia\n",
    "### Titre : {}\n",
    "\n",
    "### Article :\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    titles = examples[\"title\"]\n",
    "    texts  = examples[\"text\"]\n",
    "    outputs = []\n",
    "    for title, text in zip(titles, texts):\n",
    "        title = title or \"\"\n",
    "        text  = text or \"\"\n",
    "        formatted = wikipedia_prompt.format(title, text) + EOS_TOKEN\n",
    "        outputs.append(formatted)\n",
    "    return {\"text\": outputs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dnapk454VpCE"
   },
   "source": [
    "# Load Data and Prepare Splits\n",
    "\n",
    "* Load the French snapshot from wikimedia/wikipedia.\n",
    "* Map the formatter to produce a single text column, then create a tiny eval split from that subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 687,
     "referenced_widgets": [
      "2798a91ce5174a4785c3e0cfcf7a84e9",
      "9348ce60388d44ee9e0bbcc7352bf24a",
      "706a6d0f86404839814d0030ae4945c3",
      "e575d37453ad47d1aebe1969f3ca86ef",
      "006651d0ed6a4e459b7686b6ed2581ad",
      "5ca3b93710994b498336795c695eae68",
      "1611e274c95c492d9679cc068df586e8",
      "5877df02bd7d488780c47a9f2d448542",
      "af0ff8dbfa8a4db5b29255a1e5d9d751",
      "947bf5b1ff954d05871320c2e132929a",
      "11e73ba1d397430b8156a9e11fc9fe99",
      "c0291b8ae19d4413bd66f3d83797ef91",
      "0eeae179275a46838d1b12aeb9cb6293",
      "fabf6178d3264d6181468031a45472c0",
      "1417978fd924429b8601c7b29cdf1880",
      "0369d5957a354101b01ced383270f801",
      "80d80286a2df4c41ae3c7295f3ef7530",
      "ed8d3c286afb4d40be38587f31a84f45",
      "3e6a2a0012ea4bf695aea1b109c8f11a",
      "dd18252ff61945d2ae33b056c4ec51f0",
      "88f083c65caf406ba5547aa11a74034b",
      "95f1b37678ef4c98812fd256635ab4d0"
     ]
    },
    "id": "vO9OUZsv71we",
    "outputId": "0ac35ce5-ba25-44b1-fc3e-048a2d8d637a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2798a91ce5174a4785c3e0cfcf7a84e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0291b8ae19d4413bd66f3d83797ef91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size (within 1%): 25,389 | Eval size: 257\n",
      "\n",
      "Preview (first 500 chars):\n",
      " Article Wikipédia\n",
      "### Titre : The Ghost (film, 1913, Kirkwood)\n",
      "\n",
      "### Article :\n",
      "The Ghost est un film américain réalisé par James Kirkwood Sr., sorti en 1913.\n",
      "\n",
      "Synopsis\n",
      "\n",
      "Fiche technique \n",
      "\n",
      " Date de sortie :\n",
      "  :\n",
      "\n",
      "Distribution \n",
      " James Kirkwood Sr. : Jim\n",
      " Gertrude Robinson : Gertrude Howard\n",
      "\n",
      "Voir aussi\n",
      "\n",
      "Articles connexes \n",
      " Films américains sortis en 1913\n",
      "\n",
      "Liens externes \n",
      " \n",
      "\n",
      "Film américain sorti en 1913\n",
      "Court métrage américain\n",
      "Film dramatique américain\n",
      "Film réalisé par James Kirkwood Sr.\n",
      "Film muet amér\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "WIKI_CONFIG = \"20231101.fr\"\n",
    "\n",
    "dataset_full = load_dataset(\"wikimedia/wikipedia\", WIKI_CONFIG, split=\"train\")\n",
    "\n",
    "dataset_1p = dataset_full.train_test_split(train_size=0.01, seed=SEED)[\"train\"]\n",
    "\n",
    "dataset_1p = dataset_1p.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_1p.column_names,\n",
    "    desc=\"Formatting (title+article+EOS)\",\n",
    ")\n",
    "\n",
    "splits = dataset_1p.train_test_split(test_size=0.01, seed=SEED)\n",
    "ds_train, ds_eval = splits[\"train\"], splits[\"test\"]\n",
    "\n",
    "print(f\"Train size (within 1%): {len(ds_train):,} | Eval size: {len(ds_eval):,}\")\n",
    "print(\"\\nPreview (first 500 chars):\\n\", ds_train[0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0Vgw-r7WF8E"
   },
   "source": [
    "# Training Setup and Run\n",
    "\n",
    "* Configures UnslothTrainingArguments for a quick run: small batch with accumulation, max_steps=200 to cap time, cosine scheduler, 8-bit AdamW, and no external logging.\n",
    "* Builds UnslothTrainer over the text field and starts training on the 1% subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 691,
     "referenced_widgets": [
      "0093a8fccbf744e0b130559a204135f8",
      "73676fc15d874c0583b734d634670c4a",
      "24343afc55194e779259e4b33a556d94",
      "8ef588ec270249aaa501fdbde199e3b8",
      "fd0681c2489048ceb001431daa5c7c4b",
      "52b6af75d4df4dbd80ca6ec2473d7d50",
      "9a382a833c794e1d9e367e4daec481d2",
      "c176e164a5d04337bb816aa9ecc5251b",
      "f962686b46044dbc8d15db36805e8d72",
      "f7316db0a88642538bd8d95377817ed8",
      "6e02b3f30a044baab172b33323876734"
     ]
    },
    "id": "Ees89Vrj802c",
    "outputId": "e009b197-d4bb-451d-f770-ed38f569e59e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Switching to float32 training since model cannot work with float16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0093a8fccbf744e0b130559a204135f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=12):   0%|          | 0/25389 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 25,389 | Num Epochs = 1 | Total steps = 200\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 16\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 16 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 17,258,496 of 1,017,144,448 (1.70% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 24:23, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.267100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.109400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.162100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.209600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.262800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.258600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.282800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.281300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.312900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.296900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:270: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=2.2443505859375, metrics={'train_runtime': 1470.7136, 'train_samples_per_second': 2.176, 'train_steps_per_second': 0.136, 'total_flos': 4825366653615360.0, 'train_loss': 2.2443505859375, 'epoch': 0.12603883571625507})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "args = UnslothTrainingArguments(\n",
    "    output_dir                  = \"mistral_fr_cpt\",\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 16,\n",
    "    max_steps                   = 200,\n",
    "    learning_rate               = 5e-5,\n",
    "    embedding_learning_rate     = 5e-6,\n",
    "    warmup_ratio                = 0.1,\n",
    "    lr_scheduler_type           = \"cosine\",\n",
    "    logging_steps               = 20,\n",
    "    save_strategy               = \"steps\",\n",
    "    save_steps                  = 300,\n",
    "    optim                       = \"adamw_8bit\",\n",
    "    weight_decay                = 0.0,\n",
    "    fp16                        = not is_bfloat16_supported(),\n",
    "    bf16                        = is_bfloat16_supported(),\n",
    "    seed                        = SEED,\n",
    "    report_to                   = \"none\",\n",
    ")\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model              = model,\n",
    "    tokenizer          = tokenizer,\n",
    "    train_dataset      = ds_train,\n",
    "    eval_dataset       = None,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length     = MAX_SEQ_LEN,\n",
    "    dataset_num_proc   = 2,\n",
    "    args               = args,\n",
    ")\n",
    "\n",
    "train_stats = trainer.train()\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGn3gjQJWQ53"
   },
   "source": [
    "# Inference Helpers (After Training)\n",
    "\n",
    "* Define a helper that prints only the continuation (not the prompt) and a decode preset:  “sampled but guarded” (with repetition controls).\n",
    "* It then tests a few French prompts so you can assess post-CPT behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WpaLlAOxLLIz",
    "outputId": "c46acc60-0888-4dcf-ffbb-519c29f2ef0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Prompt ===\n",
      " Complète proprement en UNE phrase : « Une francophonie est un lieu où »\n",
      "Réponse :\n",
      "\n",
      "\n",
      "Une francopholie, une langue française et d'autres langues françaises. Le terme de Francophonie peut être utilisé pour désigner la communauté franco-canadienne ou les communautés francophones des pays du Nouveau Monde qui parlent français comme langue officielle à l’état civil (en France), ainsi que tous ceux dans le monde qui parleraient ce même français sous forme alphasyllab\n",
      "\n",
      "=== Prompt ===\n",
      " Réponds en exactement DEUX phrases sur Paris.\n",
      "Réponse :\n",
      "\n",
      "\n",
      "Paris est une ville très importante dans tous les secteurs de l'économie et des activités humaines, par exemple: la vie politique, le commerce, l’industrie, la recherche académique, l'art, l', architecture, l..., et beaucoup d'autres. Mais il y a aussi certains aspects qui peuvent être controversés ou discutables pour certaines personnes comme peut-être l'urban\n",
      "\n",
      "=== Prompt ===\n",
      " Continue ce récit en UNE à DEUX phrases : « Au cœur des Alpes, un petit village vivait au rythme des saisons. Chaque hiver, »\n",
      "Réponse :\n",
      " Un jeune alpiniste a raconté son séjour dans le village de Saint-Laurent d'Aix (Alpes). Il raconte la vie et les traditions du peuple alpinaire. L’auteur narre l’histoire du village qui se construit depuis sa fondation vers 900 pour devenir aujourd’hui une station alpine moderne avec ses chalets modernes et magnifiques ou alpages accessibles par\n"
     ]
    }
   ],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "def generate_only_new(prompt, **gen_cfg):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        return_dict_in_generate=True,\n",
    "        **gen_cfg,\n",
    "    )\n",
    "    new_tokens = out.sequences[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "SAMPLED_GUARDED_CFG = dict(\n",
    "    do_sample=True,\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.15,\n",
    "    no_repeat_ngram_size=3,\n",
    "    max_new_tokens=80,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    \"Complète proprement en une phrase : « Une francophonie est un lieu où »\\nRéponse :\",\n",
    "    \"Réponds en exactement deux phrases sur Paris.\\nRéponse :\",\n",
    "    \"Continue ce récit en une à duex phrases : « Au cœur des Alpes, un petit village vivait au rythme des saisons. Chaque hiver, »\\nRéponse :\",\n",
    "]\n",
    "\n",
    "for p in prompts:\n",
    "    print(\"\\n=== Prompt ===\\n\", p)\n",
    "    print(generate_only_new(p, **SAMPLED_GUARDED_CFG))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gQ2Oq5nQK1O"
   },
   "source": [
    "## Translations of Prompts and Responses:\n",
    "\n",
    "* Prompt 1: Complete neatly in one sentence: \"A Francophonie is a place where\"\n",
    "\n",
    "  * Response: A Francophonie, a French language, and other French languages. The term Francophonie can be used to refer to the Franco-Canadian community or the Francophone communities of New World countries that speak French as an official language in civil status (in France), as well as all those in the world who speak this same French in alphasyllabic form.\n",
    "\n",
    "* Prompt 2: Respone in exactly 2 phrases about Paris.\n",
    "\n",
    "  * Response: Paris is a very important city in all sectors of the economy and human activity, for example: political life, commerce, industry, academic research, art, architecture, and many others. But there are also certain aspects that can be controversial or debatable for some people, such as perhaps the urban planning.\n",
    "\n",
    "* Prompt 3: Continue this story in one or two sentences: \"In the heart of the Alps, a small village lived in harmony with the seasons. Every winter,\"\n",
    "\n",
    "  * Response: A young mountaineer recounted his stay in the village of Saint-Laurent d'Aix (Alps). He describes the life and traditions of the Alpine people. The author narrates the history of the village, which has developed since its founding around 900 AD and is now a modern Alpine resort with its magnificent modern chalets and accessible mountain pastures."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
