{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xucZsR3D7qFB"
   },
   "source": [
    "# Finetuning with LORA Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onVEniWL7y1H"
   },
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8W5akNiYP-Oz",
    "outputId": "20b7bdc4-a68c-4e7c-86ee-d1f236a9a7d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "unsloth 2025.11.2 requires trl!=0.19.0,<=0.23.0,>=0.18.2, but you have trl 0.24.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "unsloth-zoo 2025.11.3 requires trl!=0.19.0,<=0.24.0,>=0.18.2, but you have trl 0.25.0 which is incompatible.\n",
      "unsloth 2025.11.2 requires trl!=0.19.0,<=0.23.0,>=0.18.2, but you have trl 0.25.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install -U \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip -q install -U trl accelerate peft datasets==4.3.0 bitsandbytes transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ib4WwB0g73MV"
   },
   "source": [
    "## Imports and Hardware Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "daMJ3bIFQI2J",
    "outputId": "2d1e118b-019d-404a-9a56-68e7142c6abd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1942297853.py:1: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  import os, json, random, gc, unsloth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Torch: 2.8.0+cu126\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import os, json, random, gc, unsloth\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU detected. Use Colab > Runtime > Change runtime type > GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxHgb_3j77dj"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198,
     "referenced_widgets": [
      "c1f1077ff6ee41bfab44ff3a0e57af1e",
      "ddacc1ae6b4141f7883b57def873c29e",
      "0c3b4d9d915645a09a61d6b23b2bf3ee",
      "25175f1d9d654075928feb7916acd12e",
      "4ea5e6442ac2410db4087513df52b74a",
      "ee2c6639e8d243de82fac4dd6fe80184",
      "b91ffbfea83d4baaaf6b5b8473f76bce",
      "2121e90dc318487fbb6928cd4d47f053",
      "80a6cf391a104c7abc599653a29d5b98",
      "2c31367445814ddfac691743c4d4631b",
      "2a4f4990878c4c72900a93605506438b",
      "3df453f632704fc7845087441a101ed4",
      "633496c28fb6442c85d29178f3ad5576",
      "e3d87607c9e9417c8bc53a3fa719f72e",
      "34c340cae5704a55bc27eb9d23a5a63d",
      "6b0bf03b54434bcb88e86adbb3cc5b89",
      "d1307f5b521e487492c855ddb971932d",
      "b73d5e472dbd45368a2380f2dfdfd04c",
      "afbecb54e8284ba2bb90d7d03051a7f4",
      "007f8efd8d9e47d7b6d87b1772653901",
      "7b78677287694a47b5eb5a6319bdc803",
      "a9e8abc0b23f405f98fd18429d4450fc",
      "a23e0a18d3af433b8d20e52cb9d19dfc",
      "26c8f3303c9e45269c96f075f9530031",
      "40f1d3d13e814d8c9bab9408c0334cfa",
      "c771eccb21b74fb19d2f338f5c54c94b",
      "34bea8eae409427aa81f3ca8dcde6143",
      "4e45d59553ce4f9185fcec3ddaed8b0c",
      "e1b8977ba87341ff9e5c024ecab54178",
      "fe4f1c55cfd243a09292b2ba6a88690f",
      "ae964c6cfa9b4cdf939f3257d2e98a2d",
      "e35e1c7889db4cbd83de8d7e93f1f96d",
      "1b1deb4e725743e1bd90fe7c643b2566"
     ]
    },
    "id": "vyWlRUNpQGzs",
    "outputId": "564ec72b-cd63-4c69-a0fe-45263d366bee"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f1077ff6ee41bfab44ff3a0e57af1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df453f632704fc7845087441a101ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "code_alpaca_20k.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23e0a18d3af433b8d20e52cb9d19dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/20022 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example row: {'output': 'class Person:\\n    \"\"\"\\n    Class to represent a person\\n    \"\"\"\\n    def __init__(self, name, age, address):\\n        self.name = name\\n        self.age = age\\n        self.address = address\\n    \\n    def birthday(self):\\n        \"\"\"\\n        Increments the age of the person\\n        \"\"\"\\n        self.age += 1', 'instruction': 'Design a class for representing a person in Python.', 'input': ''}\n",
      "Train size: 2000  Test size: 200\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"sahil2801/CodeAlpaca-20k\")\n",
    "\n",
    "train_size = 2000\n",
    "test_size  = 200\n",
    "\n",
    "small_train = dataset[\"train\"].shuffle(seed=42).select(range(min(train_size, len(dataset[\"train\"]))))\n",
    "small_test  = dataset[\"train\"].shuffle(seed=123).select(range(min(test_size,  len(dataset[\"train\"]))))\n",
    "\n",
    "print(\"Example row:\", small_train[0])\n",
    "print(\"Train size:\", len(small_train), \" Test size:\", len(small_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KX5DDqnX8D4T"
   },
   "source": [
    "## Build chat messages and Templating Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7MI1KYacQRib"
   },
   "outputs": [],
   "source": [
    "def build_messages(batch):\n",
    "    user = batch[\"instruction\"]\n",
    "    if batch.get(\"input\"):\n",
    "        if isinstance(batch[\"input\"], str) and batch[\"input\"].strip():\n",
    "            user += \"\\n\\n\" + batch[\"input\"]\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "            {\"role\": \"assistant\", \"content\": batch[\"output\"]},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def apply_template(example, tokenizer, add_generation_prompt=False):\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            example[\"messages\"],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=add_generation_prompt,\n",
    "        )\n",
    "    }\n",
    "\n",
    "def preview_with_template(tokenizer, example_row):\n",
    "    ex = build_messages(example_row)\n",
    "    print(tokenizer.apply_chat_template(ex[\"messages\"], tokenize=False, add_generation_prompt=False)[:800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L85U5oGj8P9M"
   },
   "source": [
    "## Choose Chat Template and Core Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BoG1uU-6QZC1",
    "outputId": "dd7689c7-a1ad-4233-9f58-737e3b055d93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected chat template: alpaca\n"
     ]
    }
   ],
   "source": [
    "chosen_template = \"alpaca\"\n",
    "print(\"Selected chat template:\", chosen_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935,
     "referenced_widgets": [
      "db8466bd64cd41e09bfbd14ebd3d3889",
      "7f72599e0bc647839014a609fe080351",
      "57cb029595b8432cb5d741a744f9dcec",
      "8afaee90e47146919710cfaf9a5bd006",
      "e61d8b2c7cf84c54aa3a06cf52f4ad30",
      "f080707c9fa54117a205c3a6f2146d84",
      "d41caeea28744730a43d6efc0c94090a",
      "a147aa42b76546ed80a3c2ece5825753",
      "67f3061d366c49aab4e8efe8da9f151a",
      "2f05c4a32ca14f9ca225f9ca560ae719",
      "5b268bc9292141d4ab54a0a2f8359ac2",
      "1c601a9f296a4ec1a7dcd9ad33840b87",
      "fbeb2f84af92440bb4e5864e519d8a7d",
      "c5cf6b52a08c4483a3e5c986a07b3c30",
      "32db553a3ec54d658ec9ab16d4acde53",
      "8bd9017b64c849feac42fd7b20464968",
      "6a908cf1589b4880aa49b4247c85908a",
      "f1866737b9f9494eb80ea10cf18ad873",
      "013b5c2c17644b7096516f31e067d585",
      "0ba6dc52888945f3853ff532ece800de",
      "3d25cef66b1b407aa3850d634cd5089e",
      "0482a164ee5c4f13ad17bedc46b7b6db",
      "69f6fa806243482c964a85c8e715e04c",
      "d70750b9a37f46ce972470504e92e9b0",
      "5ca7fd7842004558866cb284ef35a7b5",
      "98ea90b0a7c1446ca5f5f6bb3dff44db",
      "dbeb58f41c8e4ce183649d176eaf33fe",
      "0013fbc7148649fbaa0a6c4224fa2e41",
      "fb9ea02ad305446190d79042589bc042",
      "94dca7a8435143f5b174899e6a49061a",
      "518060881fa845ecb68e9718109d21c0",
      "f71176414ddd4994b886e28f5304942b",
      "a1eb36eab8074037bb56bbddbf39b401",
      "dfc1b0cb48844a56befc239d14d2e55d",
      "9dc639e41bce418bae401b962803f321",
      "03df99d6d59949879a8a926a24bed641",
      "4a5f5d43a3b44a3c837be37d503dade6",
      "cf9d73bb1d034fbe9f939d611f175718",
      "143ba0746eee488fb64e90564232b9a2",
      "267bff6585454991bd3dcf2163354d58",
      "25ba4d0c90a4463d8d93a27c1208356f",
      "e8d307328c2146aaa1acd9d1e3c0b7aa",
      "132bc7f07fbf45d7ac041ae1f955bc9c",
      "a1c626a0e0094d3384f5ec8b9d5bcc27",
      "303fce0c3595464f8125226238bfd664",
      "781b66b936d94f37a2a25af7a83709a6",
      "0bd53cf5bd7043f8bf1d6301e017bbda",
      "d23dfc42e44c45b5b35d7f59a023b0b0",
      "caa10ac9079248ba9f29f02779a119ae",
      "9148cd0a8e194cfe87087c115613435b",
      "db680749894a44b4b428eee5e4e0fece",
      "b68c3220f3724b0d9b40c1be3421f0b2",
      "fcccfdd57ae5467088a5338b33c51c37",
      "414e771ec9a24a8783d5757647c8c6df",
      "228e95f30a3f4c2785d533597650f899",
      "14d5417ae0d944b1ac1c28c42956e545",
      "e63b40d796094938b43a4b5afe01dcd7",
      "90ce20abba424b3f95f7f06a13ff8c88",
      "6fbc112cc8b44954bed1fc35d825eb59",
      "8c4b9ca33d7c48d1b1dcc4a6277443b4",
      "96ca29fc5e8349969f93c5e3b3ea3121",
      "9a5c4f88e3f6445aabd5b052ee22e8dd",
      "8acd7f6343834473999dfb9dda8c8099",
      "5b6c169da7404ac598a14ae6f21a36fe",
      "db2c4cfc420c43f3bb3159c1f98973c3",
      "d98db8d26112400ea90b609b2a3fe0a9",
      "cf81f034e3704bfabf60cc24e7b4923f",
      "ba7c70b3f76d4e7e8d9d6c589963d252",
      "d447de6e2ee04a009b6ce8395f90de1e",
      "08712a50d792451dbbdbdacc99eac25d",
      "4bcb912bc5a942bd8afe930a606b0ef9",
      "0650a5afc5384f238deab0f44a933070",
      "1b9ba0cb32244d5c8ef4799818ea8fa5",
      "205d627ec4aa49b2a77e281fb11d6150",
      "47f5d4cb1c8f497b82fdd3140032eb4a",
      "b33e9d048c614cdd90d344f57c458b5a",
      "1669f56fea534a808e3d8493bcc779b2",
      "db268b3faeea473d9c6493db7add4bcf",
      "60937bb3b1bd41bfa0e8aa75123ddaa0",
      "65d6f3308041461080010ed6b4f96a65",
      "1d645487cf6e4e5985425fc054f7c6cd",
      "51a711cc8a4d46a0bff8171c0117261c",
      "51619b038ee84b569f0a131be3bd6bcc",
      "6b3bb1d7d131427ca5f673ea56aac3a9",
      "953aca69bc844e6a8197d8d48fd65dd9",
      "eb69ed3a71cd41dcae0e2f87968f2e0c",
      "2deefeac0c5a412bb6686b78ae6e79b2",
      "5c37194a463941c6945a4dd76517e0dd",
      "eea22e12d8524b80a61e7bd545fd6d7b",
      "88909c47fefd45838d5bccc4b5871eb5",
      "17a9204ed0da41ff81b5b03f59756b04",
      "13d10774f0d54ac4a42cd17d6adceb68",
      "1d6d6b9690db4b9380ba66f75455c84a",
      "db7d9c919c80464badb21b1671278cb4",
      "26d71188ec0844a6b461df199b96de93",
      "287bf6b1478e4e0085d293a4d3fea1c6",
      "b15764862a6b4cd6b8b8b017f247e9ed",
      "e6dcb9b688fd47d6b1b308adb232a514",
      "72bf55ca2de845218ba5f306b4eccbe4",
      "a712333453de4cd9a36f23bfd2cec31c",
      "e4c1c11abf184a5f98d5b7e0959f7b4e",
      "3f74347632354fe0aae498cc04ce28e8",
      "c11a6c8bb70b4b0390365c34c406786e",
      "09c2c23a4b9143148e405aa362cf8d06",
      "da4cc794e65048c194ea110a7a1adde8",
      "77c99b0ab0d04bc4912f63b435499919",
      "c1a96f823c514f3b8a2a8ebca8c684ef",
      "a0279bc3d8f34b8b95a471c4768ff19e",
      "6930eeba1f284f0a9bd3e80c5ae4bdc1",
      "0bc83716ff1b409dae4ce31ca7672e8a",
      "f207bf24b1d0480cacbe3b80479e1d18",
      "d28dfa3d1df342a89c54fd90d1490760",
      "c972486780b546cf878ef18df805b097",
      "efd383ea51654765a01190497e16c38b",
      "4f222c092b924646bdc3f85befe84e4a",
      "08c805ed024f41f98423005b38f4d858",
      "2bc1f5a2cb1a4d438ad03413977819f7",
      "e8204d346dec44cfbfe92235cd2f3874",
      "fb0e6f0dd1d84f549f110216a29935e0",
      "bcb46b3a50064d16a63e65d6c5b1cbfa",
      "5ab8c2f28a3c4cb6b14cf8d3c5475e2f",
      "611fb67c4c70432b9f91b830eed06b82",
      "710e5ff687314b8998546d2b32039012",
      "130869853fb045b79f2770982ef22bfb",
      "e3ede63b7b9c4ebf8a06e4ba776b6e64",
      "4df81b31a87845098cb8b3a4f375c915",
      "e4a7ac9852c64e6e97bfa8c0b199d373",
      "1b675585c19c46238cd9a72ec8bd8fe4",
      "c914d26260834f45a8c27d0c8a6895bb",
      "529978e1c6ae4047b861ebbf4f3bffb9",
      "3f38984ff6cb474ab75989aab6786642",
      "427fb8144a084b609be9f91654829d6c"
     ]
    },
    "id": "Q3BTMl9FQd-_",
    "outputId": "b823a4b9-e697-434e-87c7-d9febe714ce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8466bd64cd41e09bfbd14ebd3d3889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c601a9f296a4ec1a7dcd9ad33840b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/158 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f6fa806243482c964a85c8e715e04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc1b0cb48844a56befc239d14d2e55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303fce0c3595464f8125226238bfd664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d5417ae0d944b1ac1c28c42956e545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf81f034e3704bfabf60cc24e7b4923f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/423 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db268b3faeea473d9c6493db7add4bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.11.2 patched 30 layers with 30 QKV layers, 30 O layers and 30 MLP layers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea22e12d8524b80a61e7bd545fd6d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a712333453de4cd9a36f23bfd2cec31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f207bf24b1d0480cacbe3b80479e1d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611fb67c4c70432b9f91b830eed06b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample formatted text:\n",
      " <|im_start|>Below are some instructions that describe some tasks. Write responses that appropriately complete each request.\n",
      "\n",
      "### Instruction:\n",
      "Design a class for representing a person in Python.\n",
      "\n",
      "### Response:\n",
      "class Person:\n",
      "    \"\"\"\n",
      "    Class to represent a person\n",
      "    \"\"\"\n",
      "    def __init__(self, name, age, address):\n",
      "        self.name = name\n",
      "        self.age = age\n",
      "        self.address = address\n",
      "    \n",
      "    def birthday(self):\n",
      "        \"\"\"\n",
      "        Increments the age of the person\n",
      "        \"\"\"\n",
      "        self\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048\n",
    "dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16\n",
    "\n",
    "load_in_4bit_for_lora = False\n",
    "\n",
    "base_id = \"unsloth/SmolLM2-135M-Instruct\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=base_id,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=load_in_4bit_for_lora,\n",
    "    full_finetuning=False,\n",
    "    dtype=dtype,\n",
    ")\n",
    "\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=chosen_template)\n",
    "attn_only = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "attn_plus_mlp = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "target_modules = attn_plus_mlp\n",
    "\n",
    "# Attach LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    target_modules=target_modules,\n",
    "    use_rslora=False,\n",
    ")\n",
    "\n",
    "# Prepare tokenized text (messages -> template -> text)\n",
    "train_msgs = small_train.map(build_messages)\n",
    "test_msgs  = small_test.map(build_messages)\n",
    "train_text = train_msgs.map(lambda ex: apply_template(ex, tokenizer), remove_columns=train_msgs.column_names)\n",
    "test_text  = test_msgs.map(lambda ex: apply_template(ex, tokenizer), remove_columns=test_msgs.column_names)\n",
    "\n",
    "print(\"Sample formatted text:\\n\", train_text[0][\"text\"][:500])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tF6PAKde8zJI"
   },
   "source": [
    "## Traning Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137,
     "referenced_widgets": [
      "b87e7f0a87ba458a94bd3fb34e3b5b80",
      "35825b352c344326b21da5a3f4ac4f81",
      "52d032b1665645fc9058cfb2bc799a90",
      "4bed30894d524029b2ee7559594fd0ae",
      "8de25a26c24b4d8eb477730cf297f70f",
      "733482ccaf024dda896ba3084c9310ac",
      "de790bef1dfe4bcd8494fb38e515d166",
      "86e5d473711b4bcab40d81a46cf7719b",
      "b2c79c64afe54cb292a0ec2ef7463a44",
      "820d320cbaaa48c1b056b897b473fa3c",
      "4775c21b7828496984bcd471e4f47c05",
      "16844abb7d554e61b25e622392c281e1",
      "66a3ec89b37c40aab6971e60da4297a7",
      "b456cbdeb59f47a88d4d0d86f1741327",
      "d79c610fd1924e00a9d8834050663b8e",
      "205a8fb508b0437790118e902e2fac9c",
      "c1ee596c6bb246a6b64acae1a06ea401",
      "866021e2beba422eb41fbbf820a006a1",
      "00ef035e06164a5ba90bd257360fe4ca",
      "49a497438c364d109603b103713be329",
      "123d57bb25b04b6fb0f684edd7c9a3a0",
      "a151c0113fce4ffaa6a2c99c9c1cb1c6"
     ]
    },
    "id": "1O5W5xEXSV4p",
    "outputId": "ae6e3224-a436-4cf8-bccc-1b39820d4e44"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87e7f0a87ba458a94bd3fb34e3b5b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=12):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16844abb7d554e61b25e622392c281e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=12):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_dir = \"outputs/smollm2_lora\"\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=(dtype==torch.bfloat16),\n",
    "    fp16=(dtype==torch.float16),\n",
    "    optim=\"adamw_torch\",\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=train_args,\n",
    "    train_dataset=train_text,\n",
    "    eval_dataset=test_text,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    packing=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YV9rvv1086hr"
   },
   "source": [
    "## Train SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "BnadS_kFSa-s",
    "outputId": "e6f7f4d8-b595-4f9c-e2b4-0b9c6b9ea3cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,000 | Num Epochs = 1 | Total steps = 125\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 4,884,480 of 139,400,064 (3.50% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 02:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.749700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.149900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.879300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.828000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.836700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.804000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=1.0325559158325195, metrics={'train_runtime': 163.3519, 'train_samples_per_second': 12.244, 'train_steps_per_second': 0.765, 'total_flos': 252955228603392.0, 'train_loss': 1.0325559158325195, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "train_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lIwAMqx8_FL"
   },
   "source": [
    "## Save LoRA Adapter and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6GYTOG8nTVeM",
    "outputId": "a57ebb4e-b582-4224-853b-7bef5a17f025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: outputs/smollm2_lora\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(out_dir)\n",
    "tokenizer.save_pretrained(out_dir)\n",
    "print(\"Saved to:\", out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pg2v3Aia9G2M"
   },
   "source": [
    "## Inference Setup and Sample Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0xzV90k3TZf9",
    "outputId": "8dd342a3-1a91-4182-f832-05028a42ce16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Try asking a coding question:\n",
      "\n",
      "def two_sum(nums, target):\n",
      "    result = []\n",
      "    for i, num in enumerate(nums):\n",
      "        complement = target - num\n",
      "        if complement in result:\n",
      "            return [result.index(complement), i]\n",
      "    return []\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.eval()\n",
    "model.config.use_cache = False\n",
    "print(\"\\nTry asking a coding question:\\n\")\n",
    "\n",
    "def chat_once(prompt, temperature=0.7, top_p=0.9, max_new_tokens=256):\n",
    "    msgs = [{\"role\":\"user\",\"content\":prompt}]\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        msgs, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer([prompt_text], return_tensors=\"pt\").to(model.device)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        _ = model.generate(\n",
    "            **inputs,\n",
    "            streamer=streamer,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            use_cache=False,\n",
    "            eos_token_id=[tokenizer.eos_token_id, tokenizer.encode(\"### Instruction:\")[0]],\n",
    "        )\n",
    "\n",
    "chat_once(\"Write a Python function `two_sum(nums, target)` returning indices of two numbers that sum to target.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
