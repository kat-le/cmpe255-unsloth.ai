{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWhXO_cph16o"
   },
   "source": [
    "# Reinforcement Learning with GRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKEluWerhitw"
   },
   "source": [
    "This Colab is a reinforcement-learning demo for math word problems using GRPO (Group Relative Policy Optimization) with Unsloth. It loads a small instruction model (Qwen2.5-1.5B-Instruct) in 4-bit for low VRAM, attaches LoRA adapters, and trains the policy with two simple rewards:\n",
    "\n",
    "Format reward: outputs must follow ```<think>…</think><answer>…</answer>```.\n",
    "\n",
    "Accuracy reward: the number inside ```<answer>…</answer>``` must match the GSM8K gold answer.\n",
    "\n",
    "We’ll:\n",
    "\n",
    "* Install & check hardware (BF16/FP16).\n",
    "* Load the model (4-bit) + LoRA for efficient training.\n",
    "* Prepare a small GSM8K sample into chat messages with ground-truth numbers.\n",
    "* Define rewards (format + accuracy) and train with GRPO for a short run.\n",
    "* Generate on a new problem, then save and reload the LoRA checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0QLq0EIMAO_"
   },
   "source": [
    "# Install Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VRV66be_ep91",
    "outputId": "0f6278f6-7dee-4776-b84d-3ce6c96f8a10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth\n",
      "  Downloading unsloth-2025.11.2-py3-none-any.whl.metadata (61 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting unsloth_zoo>=2025.11.3 (from unsloth)\n",
      "  Downloading unsloth_zoo-2025.11.3-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.45.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth) (25.0)\n",
      "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.8.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.23.0+cu126)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.0.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.9.5)\n",
      "Collecting tyro (from unsloth)\n",
      "  Downloading tyro-0.9.35-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.29.5)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
      "  Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 (from unsloth)\n",
      "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.4.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\n",
      "Collecting datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1 (from unsloth)\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (1.11.0)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.17.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.36.0)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.35.2)\n",
      "Requirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.57.1)\n",
      "Collecting trl!=0.19.0,<=0.23.0,>=0.18.2 (from unsloth)\n",
      "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.20.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.32.4)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.11.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth) (0.22.1)\n",
      "Collecting torchao>=0.13.0 (from unsloth_zoo>=2025.11.3->unsloth)\n",
      "  Downloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.11.3->unsloth)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.11.3->unsloth) (11.3.0)\n",
      "Collecting msgspec (from unsloth_zoo>=2025.11.3->unsloth)\n",
      "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.0)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (0.17.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (13.9.4)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.4.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.22.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.3.1)\n",
      "Downloading unsloth-2025.11.2-py3-none-any.whl (351 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m351.3/351.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.23.0-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth_zoo-2025.11.3-py3-none-any.whl (278 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.35-py3-none-any.whl (132 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m149.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchao, shtab, pyarrow, msgspec, tyro, xformers, datasets, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, unsloth\n",
      "  Attempting uninstall: torchao\n",
      "    Found existing installation: torchao 0.10.0\n",
      "    Uninstalling torchao-0.10.0:\n",
      "      Successfully uninstalled torchao-0.10.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 18.1.0\n",
      "    Uninstalling pyarrow-18.1.0:\n",
      "      Successfully uninstalled pyarrow-18.1.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.0.0\n",
      "    Uninstalling datasets-4.0.0:\n",
      "      Successfully uninstalled datasets-4.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.48.2 cut_cross_entropy-25.1.1 datasets-4.3.0 msgspec-0.19.0 pyarrow-22.0.0 shtab-1.7.2 torchao-0.14.1 trl-0.23.0 tyro-0.9.35 unsloth-2025.11.2 unsloth_zoo-2025.11.3 xformers-0.0.32.post2\n"
     ]
    }
   ],
   "source": [
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imGEd4GHgVWl"
   },
   "outputs": [],
   "source": [
    "import platform, sys, subprocess, os, textwrap, json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uk-3fvNCNHgx"
   },
   "source": [
    "# Imports, hardware check, and runtime flags\n",
    "* Imports libraries, checks for CUDA and BF16 support, prints a short summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LgJqaxZagWYr",
    "outputId": "d6829a24-f81f-41f1-8a98-332357f2b1a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: Tesla T4\n",
      "Use BF16: False\n"
     ]
    }
   ],
   "source": [
    "import os, re, math, random, torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "try:\n",
    "    from unsloth import is_bfloat16_supported\n",
    "    BF16 = bool(is_bfloat16_supported())\n",
    "except Exception:\n",
    "    BF16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "print(\"Use BF16:\", BF16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKRSfG8wNMQJ"
   },
   "source": [
    "Set a small sample size and max prompt/completion lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ywQf0xwl26d"
   },
   "outputs": [],
   "source": [
    "sample_count = 150\n",
    "\n",
    "MAX_PROMPT_LEN      = 256\n",
    "MAX_COMPLETION_LEN  = 128   #\n",
    "\n",
    "# Disable Weights & Biases and HF logging overhead\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fslBUNDiMRde"
   },
   "source": [
    "# Load base model and attach LoRA\n",
    "\n",
    "* Loads Qwen2.5-1.5B-Instruct in 4-bit with Flash-Attention-2\n",
    "* Adds LoRA adapters to attention/MLP modules  and enables gradient checkpointing; sets tokenizer padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402,
     "referenced_widgets": [
      "d9c4dc87f75548a2b47390f12b346791",
      "b645a32234094d6da476589e7ef1d029",
      "83bf576061784cf8a57bd4b166a4cd4d",
      "e05f372b83af4a9ebe6f1830add965a7",
      "01665f5bd2c9414096dfc9a30748e191",
      "596aa728ea544ff1b9e1b95781b89589",
      "3d6cd0427c1849f4b09df5a509642785",
      "52c1792060854a8a9358886efe41a87e",
      "e16bd48a39c049ae955691e436405f34",
      "4ea43e907c4e4e57abc5f6f99a94af3d",
      "735880b01379436b8f84d23a8693f737",
      "9a2eed748b4b4c8ca96478ab5e42e4ae",
      "b4e975a438a749819514394de636fe7b",
      "3787e7159cb64a8c9b8a121ddae67818",
      "34cefb02a13748d6a67092d07915951a",
      "a0e5b45322214f449052aa0edbfee6ff",
      "15bc95fa100941b19aa04cdabbeb06f3",
      "3facf3488b5d4c87ba7a8d56047c7e7d",
      "b392f11e33dd40629397a8111b7b03e9",
      "837304357818472eac9bf712c6772868",
      "7fd21772e4164cdeb97821a2997e7d3d",
      "24dbf7be74c14e258c9e395bdc999aa4",
      "15002364d3474c899af4ed4d9f031328",
      "78560e66d6714e6082168d014d603e48",
      "571eb1eb6f5841089d10add01bc038d1",
      "0e5f9997339c4ef094e7e7ee39d405dd",
      "f086f8c24c9742aa8ae3210e72d10d16",
      "7ce3774059b247c98f7ceba3b0ab0f49",
      "d6e6fcea7af84d7faf1f576d37c2e215",
      "cfa3e4188f54477f8f583ed32edcf0f2",
      "c5e6dc6d45ea4763ad45059e03d37e15",
      "10e848e8243c44c1a1ef505e2784ad79",
      "0a57911a510449ee8f4fb6c93204e50f",
      "e0324e8f7ca54defada97f673b247907",
      "af2c686dbb5d45079951d0087be47c8e",
      "1532c3df664d4dd1bd8190d4fdc5f470",
      "e6836a6cda774be1bcdb96cac30fb12a",
      "0598ee38fb7e4b7a9814e7939b84ec45",
      "5bf0610587794d68b73181751f089824",
      "6ed6f4de9e154da0976d019dfc9730ec",
      "085b3d9b39ad4c4bb33f2d63a5c11c70",
      "eec655629e96448ab81f7c7084037962",
      "0818f322f97441ca83073831d7e3c06a",
      "6322fee674954bd6b95db8ed251b833f",
      "a2d3b1a713f343b2bba387315a6a589d",
      "7036a37a2dc64e509bffd43b1b82c91a",
      "d4ac7e02836943f39f2fcaf9ec4c589f",
      "259a9f1f12404782a30ca2d828e8f605",
      "507454964b914108b8f2e6317004304e",
      "189efdfef88f4db8ac19ec434d252e09",
      "558fbf438e5e4b0ca682ac54d120f2b9",
      "29aa402ae6674194bf530606c33fcd68",
      "f9f3bef745fb4ed49a3781c7e5cfb3ec",
      "8a72617b3ef841818620b40b9cf46923",
      "07321b1add0c4069a6609cbb1353ed56",
      "8a37b292dfdc47298c8c9322faaecb6b",
      "863f6ec6192d4ba890c46353607c8407",
      "3975c0ba6fd24384b6da45dc478ed3a0",
      "d478f6b4238146e199304d11dd26f59b",
      "5d40927e19594352a5772d62567fa7f5",
      "8259af3d299240eebda5a216ea847b1d",
      "c6a78c5ce034494d8215044d2d3d5ce4",
      "f073c319e4f8401d85d221ef0a6f3903",
      "b4c15c7e2b7b4455a248040ff706bfab",
      "a3578714bec947528a0ee5229719aafd",
      "f00679d6952e4a2e8246a1e464c3a572",
      "b83a8468b25d4587bde7079e96471c81",
      "fb495850f7a6420faa2eaae9a005f377",
      "6cc5a235ce454eafbf2a90e44d2fbd4f",
      "d69789eda53a40aab8bdd9d5ae42732e",
      "68e0cd04aaea45f4aa955397865490e7",
      "ef7644876c834c0b9379ecd02bfd73a9",
      "027cb9332b544158816b5bb6f3f0deb1",
      "0acfcd4fa5d7452c89799678900a9e85",
      "6a8f165baee24a658cfd5495aef5c179",
      "1953a038f8dd4a4cb2e0335425d09938",
      "5b777ae9f95e4175aecd560968d447c7",
      "fb3304f2ff524abb907b560c9d0b6e30",
      "21409e0faa2f409cbf55691f4981009e",
      "1b7448f99adc4ba2a8744a5961155588",
      "a86f502d6f1f49b8b085461f529c0b8a",
      "5583742f6b2f4df6b0b6bea3be1cb709",
      "a74679a9092e4775be3ad2ad0a5367b2",
      "f72e4e1828334df0b287352e977a3db1",
      "f25d6e7d7f14447b9ea909473256b181",
      "e96d4e24e5854a25921e63478cb15883",
      "b69ad2e8ac6b49359518fc618088860a",
      "628479ff82e7496fbcc0facef79cfa01"
     ]
    },
    "id": "rZzPhAjDgrja",
    "outputId": "22acbe9b-14f4-43eb-aa49-b24d18f22e01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.2: Fast Qwen2 patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c4dc87f75548a2b47390f12b346791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2eed748b4b4c8ca96478ab5e42e4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15002364d3474c899af4ed4d9f031328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0324e8f7ca54defada97f673b247907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d3b1a713f343b2bba387315a6a589d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a37b292dfdc47298c8c9322faaecb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83a8468b25d4587bde7079e96471c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3304f2ff524abb907b560c9d0b6e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.11.2 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    max_seq_length      = MAX_PROMPT_LEN + MAX_COMPLETION_LEN,\n",
    "    load_in_4bit        = True,\n",
    "    dtype               = torch.bfloat16 if BF16 else torch.float16,\n",
    "    attn_implementation = \"flash_attention_2\",\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16, lora_alpha=16, lora_dropout=0.0, bias=\"none\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f184-RPwNVyQ"
   },
   "source": [
    "# Define system prompt and single-turn chat helper\n",
    "\n",
    "* Sets a system instruction that asks the model to put reasoning in <think> and the final number in <answer>; defines chat_once(...) to build a chat prompt, generate, and return only the assistant text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEWT8AGbg4pp"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are a careful reasoning assistant. Think step by step and show your work \"\n",
    "    \"between <think></think>, then give the final numeric answer inside <answer></answer>.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWBZSExGg9DI"
   },
   "outputs": [],
   "source": [
    "def chat_once(messages, max_new_tokens=384, temperature=0.7, top_p=1.0):\n",
    "    device = model.device\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, tokenize=True, return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    decoded = tokenizer.decode(out[0][prompt.shape[-1]:], skip_special_tokens=True)\n",
    "    return decoded.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFj23XUcNio8"
   },
   "source": [
    "# Load GSM8K math problems and build a training dataset\n",
    "\n",
    "* Loads GSM8K (train), shuffles and selects up to sample_count items; extracts the gold numeric answer; converts each problem into {prompt: [system+user messages], ground_truth: <number>}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nyg7LPoihDe3",
    "outputId": "bba238c9-a81e-4b1f-e2d5-b782e474a001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': [{'content': 'You are a careful reasoning assistant. Think step by step and show your work between <think></think>, then give the final numeric answer inside <answer></answer>.', 'role': 'system'}, {'content': \"Solve step by step, then return the final numeric result in <answer></answer>.\\n\\nProblem: Ahmed is 11 years old and Fouad is 26 years old. In how many years will Fouad's age be double Ahmed's current age?\", 'role': 'user'}], 'ground_truth': '4'}\n",
      "Dataset size: 150\n"
     ]
    }
   ],
   "source": [
    "raw = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "raw = raw.shuffle(seed=SEED).select(range(min(sample_count, len(raw))))\n",
    "\n",
    "def extract_gsm8k_answer(a: str):\n",
    "    if \"####\" in a:\n",
    "        z = a.split(\"####\")[-1].strip()\n",
    "        # keep just the number-ish part\n",
    "        m = re.search(r\"-?\\d+(\\.\\d+)?\", z.replace(\",\", \"\"))\n",
    "        return m.group(0) if m else None\n",
    "    return None\n",
    "\n",
    "def to_messages(example):\n",
    "    # Conversational format (system + user) is fine for TRL GRPO\n",
    "    question = example[\"question\"].strip()\n",
    "    gt = extract_gsm8k_answer(example[\"answer\"])\n",
    "    if gt is None:\n",
    "        return None\n",
    "    msgs = [\n",
    "        {\"role\":\"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\":\"user\",   \"content\": f\"Solve step by step, then return the final numeric result in <answer></answer>.\\n\\nProblem: {question}\"},\n",
    "    ]\n",
    "    return {\"prompt\": msgs, \"ground_truth\": gt}\n",
    "\n",
    "mapped = []\n",
    "for ex in raw:\n",
    "    row = to_messages(ex)\n",
    "    if row is not None: mapped.append(row)\n",
    "\n",
    "dataset = Dataset.from_list(mapped)\n",
    "print(dataset[0])\n",
    "print(\"Dataset size:\", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zqx3LFpVN9lG"
   },
   "source": [
    "# Reward functions\n",
    "* format_reward: gives 1.0 if the reply matches ```<think>…</think><answer>…</answer>```, else 0.0.\n",
    "* accuracy_reward: parses the number inside ```<answer>…</answer>``` and gives 1.0 if it matches ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U94ozJ0VhKc-"
   },
   "outputs": [],
   "source": [
    "# 1) Format reward: must contain <think>...</think><answer>...</answer>\n",
    "_format_pat = re.compile(r\"^<think>.*?</think>\\s*<answer>.*?</answer>\\s*$\", re.S)\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "    # completions: list[list[{\"role\":\"assistant\",\"content\": \"...\"}]]\n",
    "    contents = [c[0][\"content\"] if isinstance(c, list) else str(c) for c in completions]\n",
    "    return [1.0 if _format_pat.match(txt.strip()) else 0.0 for txt in contents]\n",
    "\n",
    "# 2) Accuracy reward: pull the content inside <answer>...</answer> and compare to GT\n",
    "def _extract_answer_field(s: str):\n",
    "    m = re.search(r\"<answer>\\s*(.*?)\\s*</answer>\", s, re.S)\n",
    "    if m:\n",
    "        payload = m.group(1).strip()\n",
    "        n = re.search(r\"-?\\d+(\\.\\d+)?\", payload.replace(\",\", \"\"))\n",
    "        return n.group(0) if n else None\n",
    "    if \"####\" in s:\n",
    "        z = s.split(\"####\")[-1]\n",
    "        n = re.search(r\"-?\\d+(\\.\\d+)?\", z.replace(\",\", \"\"))\n",
    "        return n.group(0) if n else None\n",
    "    return None\n",
    "\n",
    "def accuracy_reward(completions, ground_truth, **kwargs):\n",
    "    contents = [c[0][\"content\"] if isinstance(c, list) else str(c) for c in completions]\n",
    "    preds = [_extract_answer_field(t) for t in contents]\n",
    "    rewards = []\n",
    "    for p, g in zip(preds, ground_truth):\n",
    "        rewards.append(1.0 if (p is not None and g is not None and p == g) else 0.0)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySuXSUjvOWm0"
   },
   "source": [
    "# Build GRPO trainer and train\n",
    "\n",
    "* Instantiates GRPOTrainer with the model, both reward functions, tokenizer, and dataset; starts training with .train().\n",
    "* The trainer samples, scores with the rewards, and updates the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCdBUCnJhQLF"
   },
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "NUM_GENERATIONS = 2\n",
    "BATCH_PER_DEVICE = 2\n",
    "GA_STEPS = 1\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir                    = \"grpo-fast\",\n",
    "    learning_rate                 = 5e-6,\n",
    "    weight_decay                  = 0.1,\n",
    "    warmup_ratio                  = 0.1,\n",
    "    lr_scheduler_type             = \"cosine\",\n",
    "    optim                         = \"adamw_8bit\",\n",
    "    logging_steps                 = 10,\n",
    "    report_to                     = \"none\",\n",
    "    save_strategy                 = \"no\",\n",
    "    bf16                          = BF16,\n",
    "    fp16                          = (not BF16),\n",
    "    per_device_train_batch_size   = BATCH_PER_DEVICE,\n",
    "    gradient_accumulation_steps   = GA_STEPS,\n",
    "    num_generations               = NUM_GENERATIONS,\n",
    "    max_steps                     = 100,\n",
    "    temperature                   = 1.0,\n",
    "    top_p                         = 1.0,\n",
    "    max_prompt_length             = MAX_PROMPT_LEN,\n",
    "    max_completion_length         = MAX_COMPLETION_LEN,\n",
    "    loss_type                     = \"dapo\",\n",
    "    epsilon_high                  = 0.28,\n",
    "    beta                          = 0.0,\n",
    "    mask_truncated_completions    = True,\n",
    "    use_vllm                      = False,\n",
    "    dataloader_num_workers        = 2,\n",
    ")\n",
    "\n",
    "\n",
    "from trl import GRPOTrainer\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model            = model,\n",
    "    reward_funcs     = [format_reward, accuracy_reward],\n",
    "    args             = training_args,\n",
    "    train_dataset    = dataset,\n",
    "    processing_class = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "Im69_DtAiXyT",
    "outputId": "8deddb14-cce1-4725-a598-40d03c587f92"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 150 | Num Epochs = 1 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 1 x 1) = 2\n",
      " \"-____-\"     Trainable parameters = 18,464,768 of 1,562,179,072 (1.18% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 15:10, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>sampling / sampling_logp_difference / mean</th>\n",
       "      <th>sampling / sampling_logp_difference / max</th>\n",
       "      <th>sampling / importance_sampling_ratio / min</th>\n",
       "      <th>sampling / importance_sampling_ratio / mean</th>\n",
       "      <th>sampling / importance_sampling_ratio / max</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / format_reward / mean</th>\n",
       "      <th>rewards / format_reward / std</th>\n",
       "      <th>rewards / accuracy_reward / mean</th>\n",
       "      <th>rewards / accuracy_reward / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>126.950000</td>\n",
       "      <td>125.900000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.070711</td>\n",
       "      <td>124.850000</td>\n",
       "      <td>121.700000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>19.300000</td>\n",
       "      <td>19.300000</td>\n",
       "      <td>19.300000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.070711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.070711</td>\n",
       "      <td>124.850000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>126.700000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>21.400000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.070711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>125.700000</td>\n",
       "      <td>123.400000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>33.800000</td>\n",
       "      <td>33.800000</td>\n",
       "      <td>33.800000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>126.300000</td>\n",
       "      <td>124.600000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.141421</td>\n",
       "      <td>126.200000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>23.050000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.100000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.070711</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.070711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>126.200000</td>\n",
       "      <td>124.400000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.141421</td>\n",
       "      <td>124.350000</td>\n",
       "      <td>120.700000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>18.300000</td>\n",
       "      <td>18.300000</td>\n",
       "      <td>18.300000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.070711</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.070711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=1.6038306682730763e-10, metrics={'train_runtime': 926.6675, 'train_samples_per_second': 0.216, 'train_steps_per_second': 0.108, 'total_flos': 0.0, 'train_loss': 1.6038306682730763e-10})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CS3uUzQP2kj"
   },
   "source": [
    "# Test generation after training\n",
    "\n",
    "* Asks a new marble-ratio problem using the same system prompt; calls chat_once(...) to generate a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UCs1zHrYqISo",
    "outputId": "9aef7d15-fad0-49a9-a00a-fad1fa9adbce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's denote the number of additional red marbles added as \\( x \\).\n",
      "\n",
      "Initially:\n",
      "- Red marbles = 18\n",
      "- Blue marbles = 12\n",
      "\n",
      "After adding \\( x \\) red marbles:\n",
      "- New total red marbles = 18 + \\( x \\)\n",
      "- Blue marbles remain unchanged at 12.\n",
      "\n",
      "According to the problem, after this addition, the ratio of red marbles to blue marbles should be 5:3. So we can write:\n",
      "\n",
      "\\[\n",
      "\\frac{18 + x}{12} = \\frac{5}{3}\n",
      "\\]\n",
      "\n",
      "To solve for \\( x \\), cross-multiply:\n",
      "\n",
      "\\[\n",
      "3(18 + x) = 5 \\times 12\n",
      "\\]\n",
      "\n",
      "Simplify both sides:\n",
      "\n",
      "\\[\n",
      "54 + 3x = 60\n",
      "\\]\n",
      "\n",
      "Subtract 54 from both sides:\n",
      "\n",
      "\\[\n",
      "3x = 6\n",
      "\\]\n",
      "\n",
      "Divide both sides by 3:\n",
      "\n",
      "\\[\n",
      "x = 2\n",
      "\\]\n",
      "\n",
      "So, the value of \\( x \\) is 2. The final answer is:\n",
      "\n",
      "<answer>2</answer>\n"
     ]
    }
   ],
   "source": [
    "test_messages = [\n",
    "    {\"role\":\"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\":\"user\",   \"content\": \"A jar has 18 red and 12 blue marbles. If you add x red marbles so that red:blue becomes 5:3, what is x? Return final answer in <answer></answer>.\"},\n",
    "]\n",
    "print(chat_once(test_messages, max_new_tokens=384, temperature=0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9J7euJL1P8j1"
   },
   "source": [
    "# Save LoRA adapter and tokenizer\n",
    "\n",
    "* Saves the fine-tuned LoRA weights and tokenizer files to grpo_saved_lora/ so you can reuse/deploy the tuned policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s4pcICFErEmd",
    "outputId": "b4e37bae-c87c-4cac-a08b-32d9860dc34d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.2: Fast Qwen2 patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Loaded LoRA-only checkpoint with Unsloth.\n"
     ]
    }
   ],
   "source": [
    "OUT_DIR = \"grpo_saved_lora\"\n",
    "model.save_pretrained(OUT_DIR)\n",
    "tokenizer.save_pretrained(OUT_DIR)\n",
    "\n",
    "lora_model, lora_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    OUT_DIR,\n",
    "    max_seq_length      = MAX_SEQ_LEN,\n",
    "    load_in_4bit        = True,\n",
    "    dtype               = torch.bfloat16 if BF16 else torch.float16,\n",
    "    attn_implementation = \"flash_attention_2\",\n",
    ")\n",
    "print(\"Loaded LoRA-only checkpoint with Unsloth.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
